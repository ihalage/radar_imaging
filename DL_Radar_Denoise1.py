import numpy as np
import glob
import os
import matplotlib.pyplot as plt
import tensorflow as tf
# import cv2
from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise
from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D, Lambda
from keras.layers import MaxPooling2D, concatenate
from keras.layers.advanced_activations import LeakyReLU
from keras.layers.convolutional import UpSampling2D, Conv2D, Conv2DTranspose
from keras.models import Sequential, Model
from keras.optimizers import Adam
from keras import losses
from keras.utils import to_categorical
import keras.backend as K
from keras.callbacks import ModelCheckpoint
from sklearn.model_selection import train_test_split
# from numpy import unravel_index
# import math
import time
start_time = time.time()


CON_DIRECTORY='Train Libraray Experiment/image_CON_1'		#data shape (12x12)
CS_DIRECTORY='Train Libraray Experiment/image_CS_1'	#image shape (100x100)


class DL_Radar():

	def __init__(self):
		self.no_of_images = 3537
		self.in_shape = (100,100,1)	# shape of the input

		# dimensions for normalizing and reshaping purposes in functions read_magnitude() and read_phase()
		self.input_area = 100*100
		self.input_dimension = 100
		self.target_area = 100*100
		self.target_dimension = 100
		##############################################################################################

		# uncomment when the arrays are not loaded from a directory
		self.real_data,self.imaginary_data = [i.reshape(self.no_of_images*self.input_dimension*self.input_dimension,1) for i in (self.unzip(list(self.read_data(CON_DIRECTORY))))]
		self.real_image,self.imaginary_image = [i.reshape(self.no_of_images*self.target_dimension*self.target_dimension,1) for i in (self.unzip(list(self.read_data(CS_DIRECTORY))))]

		self.input_data = np.sqrt(self.real_data**2+self.imaginary_data**2)
		self.target_data = np.sqrt(self.real_image**2+self.imaginary_image**2)

		self.input_phase_indata = np.arctan2(self.imaginary_data * 1.0,self.real_data)
		self.target_phase_indata = np.arctan2(self.imaginary_image * 1.0,self.real_image)

	def read_data(self,directory):
		# os.chdir(directory)
		# for file in glob.glob('*.txt'):
		# 	data = np.loadtxt(file, delimiter=',')
		for root, dirs, files in os.walk(directory):
			for file in files:
				# print file
				if file.endswith(".txt"):
					data = np.loadtxt(os.path.join(root, file), delimiter=',')
					# print data.shape
					real_data = data[0:data.shape[0]/2]
					imaginary_data = data[data.shape[0]/2:]
					
					yield real_data,imaginary_data



	#to assign the two values generated by yield generator to 2 different variables
	def unzip(self, b):
		xs, ys = zip(*b)
		xs = np.array(xs)
		ys = np.array(ys)
		return xs, ys


	def normalize(self, in_array):
		max_val = max(in_array)
		min_val = min(in_array)
		norm_array = (in_array - min_val) * 1.0 / (max_val - min_val)
		return norm_array


	# read the magnitudes and reshape
	def read_magnitude(self, inputs, targets):
		for i in range(self.no_of_images):
			normalized_inputs = self.normalize(inputs[i*self.input_area:i*self.input_area+self.input_area])
			normalized_targets = self.normalize(targets[i*self.target_area:i*self.target_area+self.target_area])

			yield (normalized_inputs.reshape(self.input_dimension,self.input_dimension).T.reshape(self.input_dimension,self.input_dimension,1), 
					normalized_targets.reshape(self.target_dimension,self.target_dimension).T.reshape(self.target_dimension,self.target_dimension,1))

	# read the phases and reshape
	def read_phase(self, inputs, targets):
		for i in range(self.no_of_images):
			normalized_inputs_phase = self.normalize(inputs[i*self.input_area:i*self.input_area+self.input_area])
			normalized_targets_phase = self.normalize(targets[i*self.target_area:i*self.target_area+self.target_area])

			yield (normalized_inputs_phase.reshape(self.input_dimension,self.input_dimension).T.reshape(self.input_dimension,self.input_dimension,1), 
					normalized_targets_phase.reshape(self.target_dimension,self.target_dimension).T.reshape(self.target_dimension,self.target_dimension,1))


	def build_network(self, in_shape):

		magnitude = Input(shape=in_shape, name='magnitude')
		# phase = Input(shape=in_shape, name='phase')

		## magnitude processing neural network
		mag_conv1 = Conv2D(32, [5,5], strides=(2,2), padding='same', activation='relu', name='mag_conv1')(magnitude) #50x50
		mag_conv2 = Conv2D(48, [5,5], strides=(2,2), padding='same', activation='relu', name='mag_conv2')(mag_conv1) #25x25
		mag_conv3 = Conv2D(64, [5,5], strides=(5,5), padding='same', activation='relu', name='mag_conv3')(mag_conv2) #5x5

		dconv1 = Conv2DTranspose(64, [5,5], strides=(5,5), padding='same', activation='relu', name='dconv1')(mag_conv3) #25x25
		dconv2 = Conv2DTranspose(32, [5,5], strides=(2,2), padding='same', activation='relu', name='dconv2')(dconv1) #50x50
		dconv3 = Conv2DTranspose(1, [5,5], strides=(2,2), padding='same', activation='relu', name='dconv3')(dconv2) #100x100


		dl_model = Model(inputs=[magnitude], outputs=[dconv3])

		return dl_model

	def train(self, epochs, learn_rate, batch_size=20):

		# data preparation, with normalization
		all_xs, all_ys = self.unzip(list(self.read_magnitude(self.input_data,self.target_data)))
		# all_xs = np.load('saved_arrays/Denoise1all_all_xs.npy')
		# all_ys = np.load('saved_arrays/Denoise1all_all_ys.npy')

		np.save('saved_arrays/Denoise1all_all_xs', all_xs)
		np.save('saved_arrays/Denoise1all_all_ys', all_ys)

		all_phase_xs, all_phase_ys = self.unzip(list(self.read_phase(self.input_phase_indata,self.target_phase_indata)))
		# all_phase_xs = np.load('saved_arrays/Denoise1all_all_phase_xs.npy')
		# all_phase_ys = np.load('saved_arrays/Denoise1all_all_phase_ys.npy')

		np.save('saved_arrays/Denoise1all_all_phase_xs', all_phase_xs)
		np.save('saved_arrays/Denoise1all_all_phase_ys', all_phase_ys)

		self.train_xs, self.test_xs, self.train_ys, self.test_ys  = train_test_split(all_xs, all_ys, test_size=0.1, random_state=14)
		self.phase_train_xs, self.phase_test_xs, self.phase_train_ys, self.phase_test_ys  = train_test_split(all_phase_xs, all_phase_ys, test_size=0.1, random_state=14)
# 

		self.network = self.build_network(self.in_shape)
		adam = Adam(lr=learn_rate)

		self.network.compile(optimizer=adam, loss='mean_squared_error', metrics=['mse'])
		print (self.network.summary())


		def do_report(e):
		
			predictions = self.network.predict([self.test_xs])
			# plt.imshow(test_batch_ys.reshape(60,60))
			# plt.show()
			for i in range(50):
				f = plt.figure(figsize=(8,8))
				ax = f.add_subplot(2,2,1)
				ax.set_title("Conventional Image", fontsize=11)
				plt.imshow(self.test_xs[i].reshape(self.input_dimension,self.input_dimension))
				ax = f.add_subplot(2,2,4)
				ax.set_title("Denoised Prediction", fontsize=11)
				plt.imshow(predictions[i].reshape(self.target_dimension,self.target_dimension))
				
				# # plt.subplots_adjust(bottom=0.1, right=0.8, top=1)
				# ax = f.add_subplot(2,2,3)
				# ax.set_title("Actual Target", fontsize=11)
				# plt.imshow(self.test_ys[i].reshape(self.target_dimension,self.target_dimension))
				# predicted_target = np.zeros((self.target_dimension,self.target_dimension))
				# # x,y = unravel_index(r[0][i].reshape(60,60).argmax(), (60,60))
				# # predicted_target[x][y] = 1.0
				# # plt.subplots_adjust(bottom=0.1, right=0.8, top=1)
				ax = f.add_subplot(2,2,2)
				ax.set_title("Compressive Sensing Image", fontsize=11)
				plt.imshow(self.test_ys[i].reshape(self.input_dimension,self.input_dimension))
				plt.savefig('Scheme5_Denoise/epoch'+str(e)+'_test'+str(i))
				# plt.show()
				plt.close()

		checkpointer = ModelCheckpoint('saved_models/radarDenoise1_{epoch:0003d}.h5', period=20)
# 
		try:
			for epoch in range(epochs):
				self.network.fit([self.train_xs], [self.train_ys], validation_split=0.2, epochs=1, batch_size=batch_size, callbacks=[checkpointer])
				if epoch%20 ==0 and epoch!=0:
					do_report(epoch)

		except KeyboardInterrupt:
			print('############# Training interrupted ###########')


if __name__ == '__main__':
	dl_radar = DL_Radar()
	dl_radar.train(epochs=1000, learn_rate=0.001, batch_size=20)

